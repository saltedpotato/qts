{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "\n",
    "from data.cons_data import get_cons\n",
    "from data.market_data import market_data\n",
    "\n",
    "from utils.market_time import market_hours\n",
    "from utils.params import PARAMS\n",
    "from utils.clustering_methods import Clustering_methods\n",
    "\n",
    "from pairs_finding.pairs_identification import cointegration_pairs\n",
    "from pairs_finding.clustering import Clustering\n",
    "\n",
    "from trade.pairs_trader import PairsTrader\n",
    "from trade.optimizer import optimizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    (\"GOOGL\", \"GOOG\"): {\n",
    "        PARAMS.beta_win: 100,\n",
    "        PARAMS.z_win: 10,\n",
    "        PARAMS.z_entry: 2,\n",
    "        PARAMS.z_exit: 1,\n",
    "        PARAMS.trade_freq: \"1m\",\n",
    "    },\n",
    "    (\"GOOGL\", \"AAPL\"): {\n",
    "        PARAMS.beta_win: 100,\n",
    "        PARAMS.z_win: 10,\n",
    "        PARAMS.z_entry: 2,\n",
    "        PARAMS.z_exit: 1,\n",
    "        PARAMS.trade_freq: \"5m\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "etf = \"QQQ\"\n",
    "cons = get_cons(etf=etf)\n",
    "cons_date = cons.read()\n",
    "\n",
    "data = market_data(\n",
    "    file_path=\"C:/Users/edmun/OneDrive/Desktop/Quantitative Trading Strategies/Project/qts/data/polygon/*.parquet\"\n",
    ")\n",
    "out_path = \"output/polygon\"\n",
    "earliest_date_year = [\n",
    "    i\n",
    "    for i in cons_date.keys()\n",
    "    if datetime.strptime(i, \"%Y-%m-%d\").date()\n",
    "    >= datetime.strptime(\"2020-06-30\", \"%Y-%m-%d\").date()\n",
    "]\n",
    "\n",
    "periods = 30\n",
    "\n",
    "period_ends = (\n",
    "    pl.DataFrame(earliest_date_year, schema=[\"Date\"])\n",
    "    .with_columns(\n",
    "        pl.all().cast(pl.Date),\n",
    "    )\n",
    "    .with_columns((pl.col(\"Date\").rank() // periods).alias(\"Chunk\"))\n",
    "    .group_by(\"Chunk\", maintain_order=True)\n",
    "    .agg(pl.col(\"Date\").last())[\"Date\"]\n",
    "    .dt.strftime(\"%Y-%m-%d\")\n",
    "    .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "for i in range(10, len(period_ends)):  # range(2, len(period_ends))\n",
    "    warm_start, train_start, train_end, trade_end = (\n",
    "        period_ends[i - 10],\n",
    "        period_ends[i - 2],\n",
    "        period_ends[i - 1],\n",
    "        period_ends[i],\n",
    "    )\n",
    "\n",
    "    print(warm_start, train_start, train_end, trade_end)\n",
    "    # next trading day\n",
    "    last_date = datetime.strptime(train_end, \"%Y-%m-%d\")\n",
    "    next_day = (last_date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if os.path.isfile((f\"{out_path}/result/result_{next_day}_{trade_end}.csv\")):\n",
    "        continue\n",
    "    # TRAINING PERIOD FINDING OPTIMAL PARAMS #\n",
    "    data.read(cons=cons_date[train_end], start=train_start, end=train_end)\n",
    "\n",
    "    train = data.filter(resample_freq=\"15m\", hours=market_hours.MARKET)\n",
    "\n",
    "    c = Clustering(df=train.select(pl.all().exclude([\"date\", \"time\"])))\n",
    "\n",
    "    # c.run_clustering(method=Clustering_methods.kmeans, min_clusters=2, max_clusters=6)\n",
    "\n",
    "    c.run_clustering(method=Clustering_methods.agnes, min_clusters=2, max_clusters=5)\n",
    "\n",
    "    find_pairs = cointegration_pairs(\n",
    "        df=train.select(pl.all().exclude([\"date\", \"time\"])),\n",
    "        p_val_cutoff=0.005,\n",
    "        cluster_pairs=c.cluster_pairs,\n",
    "    )\n",
    "    find_pairs.identify_pairs()\n",
    "    for_j = find_pairs.get_top_pairs()\n",
    "\n",
    "    output[f\"{train_start}_{train_end}\"] = for_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/edmun/Downloads/j.json\", \"w\") as json_file:\n",
    "    json.dump(output, json_file, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10, len(period_ends)):  # range(2, len(period_ends))\n",
    "    warm_start, train_start, train_end, trade_end = (\n",
    "        period_ends[i - 10],\n",
    "        period_ends[i - 2],\n",
    "        period_ends[i - 1],\n",
    "        period_ends[i],\n",
    "    )\n",
    "\n",
    "    print(warm_start, train_start, train_end, trade_end)\n",
    "    # next trading day\n",
    "    last_date = datetime.strptime(train_end, \"%Y-%m-%d\")\n",
    "    next_day = (last_date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if os.path.isfile((f\"{out_path}/result/result_{next_day}_{trade_end}.csv\")):\n",
    "        continue\n",
    "    # TRAINING PERIOD FINDING OPTIMAL PARAMS #\n",
    "    data.read(cons=cons_date[train_end], start=train_start, end=train_end)\n",
    "\n",
    "    train = data.filter(resample_freq=\"15m\", hours=market_hours.MARKET)\n",
    "\n",
    "    c = Clustering(df=train.select(pl.all().exclude([\"date\", \"time\"])))\n",
    "\n",
    "    # c.run_clustering(method=Clustering_methods.kmeans, min_clusters=2, max_clusters=6)\n",
    "\n",
    "    c.run_clustering(method=Clustering_methods.agnes, min_clusters=2, max_clusters=5)\n",
    "\n",
    "    find_pairs = cointegration_pairs(\n",
    "        df=train.select(pl.all().exclude([\"date\", \"time\"])),\n",
    "        p_val_cutoff=0.005,\n",
    "        cluster_pairs=c.cluster_pairs,\n",
    "    )\n",
    "    find_pairs.identify_pairs()\n",
    "\n",
    "    potential_pairs = [\n",
    "        pair[0]\n",
    "        for sublist in find_pairs.cluster_sorted_pairs.values()\n",
    "        for pair in sublist\n",
    "    ]\n",
    "    data.read(\n",
    "        cons=set([item for pair in potential_pairs for item in pair]),\n",
    "        start=warm_start,\n",
    "        end=train_end,\n",
    "    )\n",
    "\n",
    "    opt = optimizer(\n",
    "        data=data,\n",
    "        find_pairs=find_pairs,  # list(params.keys()), # pairs_to_trade\n",
    "        start=pl.lit(train_start).str.strptime(pl.Date, \"%Y-%m-%d\"),\n",
    "        end=pl.lit(train_end).str.strptime(pl.Date, \"%Y-%m-%d\"),\n",
    "    )\n",
    "\n",
    "    study = opt.optimize(\n",
    "        study_name=\"PAIRS_TRADING\",\n",
    "        output_file_name=f\"{out_path}/db/result_{next_day}_{trade_end}.db\",\n",
    "        n_trials=10,\n",
    "    )\n",
    "    p = study.best_params\n",
    "\n",
    "    study.trials_dataframe().to_csv(\n",
    "        f\"{out_path}/trials/trials_{train_start}_{train_end}.csv\"\n",
    "    )\n",
    "\n",
    "    optimal_params = {}\n",
    "    for key, value in p.items():\n",
    "        if key not in [\"pairs_to_trade\", \"buffer_capital\"]:\n",
    "            parts = key.split(\"_\")\n",
    "\n",
    "            pair = (parts[0], parts[1])\n",
    "            param_name = \"_\".join(parts[2:])\n",
    "\n",
    "            if pair not in optimal_params:\n",
    "                optimal_params[pair] = {}\n",
    "\n",
    "            optimal_params[pair][param_name] = value\n",
    "\n",
    "    # TRADING PERIOD USING PARAMS\n",
    "\n",
    "    # reading pairs only from next trading day to next q end\n",
    "    pairs_to_trade = list(optimal_params.keys())\n",
    "    data.read(\n",
    "        cons=set([item for pair in pairs_to_trade for item in pair]),\n",
    "        start=train_start,\n",
    "        end=trade_end,\n",
    "    )\n",
    "\n",
    "    trader = PairsTrader(\n",
    "        data=data,\n",
    "        pairs=pairs_to_trade,  # list(params.keys()),  # pairs_to_trade\n",
    "        params=optimal_params,\n",
    "        trade_hour=market_hours.MARKET,\n",
    "    )\n",
    "\n",
    "    pl_next_day = pl.lit(next_day).str.strptime(pl.Date, \"%Y-%m-%d\")\n",
    "    pl_trade_end = pl.lit(trade_end).str.strptime(pl.Date, \"%Y-%m-%d\")\n",
    "    returns = trader.backtest(\n",
    "        start=pl_next_day,\n",
    "        end=pl_trade_end,\n",
    "        cost=0.0005,\n",
    "        stop_loss=None,\n",
    "        # np.array(\n",
    "        #     [optimal_params[(p1, p2)][PARAMS.stop_loss] for p1, p2 in pairs_to_trade]\n",
    "        # ),\n",
    "    )\n",
    "\n",
    "    returns.with_columns(\n",
    "        pl.col(\"CAPITAL\").pct_change().fill_null(0).alias(\"PORT_RET\")\n",
    "    ).write_csv(f\"{out_path}/result/result_{next_day}_{trade_end}.csv\")\n",
    "\n",
    "    convert_json = {f\"{p1}_{p2}\": params for (p1, p2), params in optimal_params.items()}\n",
    "    convert_json[\"pairs_to_trade\"] = p[\"pairs_to_trade\"]\n",
    "    convert_json[\"buffer_capital\"] = p[\"buffer_capital\"]\n",
    "    with open(\n",
    "        f\"{out_path}/params/optimal_params_{next_day}_{trade_end}.json\", \"w\"\n",
    "    ) as json_file:\n",
    "        json.dump(convert_json, json_file, default=str)\n",
    "\n",
    "    del c, opt, find_pairs, trader  # free ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    \"pairs_to_trade\": 1,\n",
    "    \"CPRT_SBUX_beta_win\": 1000,\n",
    "    \"CPRT_SBUX_hurst_win\": 1000,\n",
    "    \"CPRT_SBUX_z_win\": 1000,\n",
    "    # \"CPRT_SBUX_z_win\": 50,\n",
    "    \"CPRT_SBUX_z_entry\": 3.0,\n",
    "    \"CPRT_SBUX_z_exit\": -1.7,\n",
    "    \"CPRT_SBUX_trade_freq\": \"7m\",\n",
    "    \"CPRT_SBUX_stop_loss\": 0.01,\n",
    "    \"CPRT_SBUX_z_stop_scaler\":1.2,\n",
    "    \"MRNA_TSLA_beta_win\": 1000,\n",
    "    \"MRNA_TSLA_hurst_win\": 10,\n",
    "    \"MRNA_TSLA_z_win\": 1000,\n",
    "    # \"MRNA_TSLA_z_win\": 5,\n",
    "    \"MRNA_TSLA_z_entry\": 2.8,\n",
    "    \"MRNA_TSLA_z_exit\": -2.5,\n",
    "    \"MRNA_TSLA_trade_freq\": \"1m\",\n",
    "    \"MRNA_TSLA_stop_loss\": 0.004,\n",
    "    \"MRNA_TSLA_z_stop_scaler\":1.2,\n",
    "}\n",
    "\n",
    "train_start, train_end, trade_end = \"2020-06-30\", \"2022-12-31\", \"2020-08-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = {}\n",
    "for key, value in p.items():\n",
    "    if key != \"pairs_to_trade\":\n",
    "        parts = key.split(\"_\")\n",
    "\n",
    "        pair = (parts[0], parts[1])\n",
    "        param_name = \"_\".join(parts[2:])\n",
    "\n",
    "        if pair not in optimal_params:\n",
    "            optimal_params[pair] = {}\n",
    "\n",
    "        optimal_params[pair][param_name] = value\n",
    "\n",
    "# TRADING PERIOD USING PARAMS\n",
    "# next trading day\n",
    "last_date = datetime.strptime(train_end, \"%Y-%m-%d\")\n",
    "next_day = (last_date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# reading pairs only from next trading day to next q end\n",
    "pairs_to_trade = list(optimal_params.keys())\n",
    "\n",
    "data.read(\n",
    "    cons=set([item for pair in pairs_to_trade for item in pair]),\n",
    "    start=train_start,\n",
    "    end=train_end,\n",
    ")\n",
    "\n",
    "trader = PairsTrader(\n",
    "    data=data,\n",
    "    pairs=pairs_to_trade,  # list(params.keys()),  # pairs_to_trade\n",
    "    params=optimal_params,\n",
    "    trade_hour=market_hours.MARKET,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trader.generate_backtest_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_next_day = pl.lit(train_start).str.strptime(pl.Date, \"%Y-%m-%d\")\n",
    "pl_trade_end = pl.lit(train_end).str.strptime(pl.Date, \"%Y-%m-%d\")\n",
    "returns = trader.backtest(\n",
    "    start=pl_next_day,\n",
    "    end=pl_trade_end,\n",
    "    cost=0.0005,\n",
    "    stop_loss=np.array(\n",
    "        [optimal_params[(p1, p2)][PARAMS.stop_loss] for p1, p2 in pairs_to_trade]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trader = PairsTrader(\n",
    "    data=data,\n",
    "    pairs=pairs_to_trade,  # list(params.keys()),  # pairs_to_trade\n",
    "    params=optimal_params,\n",
    "    trade_hour=market_hours.MARKET,\n",
    ")\n",
    "\n",
    "pl_next_day = pl.lit(train_start).str.strptime(pl.Date, \"%Y-%m-%d\")\n",
    "pl_trade_end = pl.lit(train_end).str.strptime(pl.Date, \"%Y-%m-%d\")\n",
    "returns = trader.backtest(\n",
    "    start=pl_next_day,\n",
    "    end=pl_trade_end,\n",
    "    cost=0.0005,\n",
    "    stop_loss=np.array(\n",
    "        [optimal_params[(p1, p2)][PARAMS.stop_loss] for p1, p2 in pairs_to_trade]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((returns.select(\"CAPITAL\")\n",
    "            .with_columns(pl.all().pct_change())\n",
    "            .fill_null(0)\n",
    "            .to_numpy()\n",
    "            .flatten()\n",
    "        )+1).prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.write_csv(\"check.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trader.generate_backtest_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_arr = df.select(\n",
    "    # select reorders the columns\n",
    "    [f\"Z_MRNA_ON_TSLA\", \"Z_CPRT_ON_SBUX\"]\n",
    ").to_numpy()  # shape: n rows, n pairs\n",
    "\n",
    "beta_arr = df.select(\n",
    "    # select reorders the columns\n",
    "    [f\"BETA_MRNA_ON_TSLA\", \"Z_CPRT_ON_SBUX\"]\n",
    ").to_numpy()  # shape: n rows, n pairs\n",
    "\n",
    "# hurst_arr = df.select(\n",
    "#     # select reorders the columns\n",
    "#     [f\"HURST_MRNA_ON_TSLA\" for p1, p2 in self.pairs]\n",
    "# ).to_numpy()  # shape: n rows, n pairs\n",
    "\n",
    "# market_close_flag = df.select(\"market_close\").to_numpy().flatten()\n",
    "\n",
    "z_entry_arr = np.array([1, 1])\n",
    "z_exit_arr = np.array([0.6, 2])\n",
    "\n",
    "signal_arr, pos_arr, pos_beta_arr = trader.compute_pos(\n",
    "    Z_arr=Z_arr,\n",
    "    beta_arr=beta_arr,\n",
    "    # hurst_arr=hurst_arr,\n",
    "    n_pairs=2,\n",
    "    z_entry_arr=z_entry_arr,\n",
    "    z_exit_arr=z_exit_arr,\n",
    "    market_close_flag=np.zeros(len(df)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame(pos_arr).write_csv(\"hmm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.isnan(pos_arr)\n",
    "idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n",
    "np.maximum.accumulate(idx, axis=1, out=idx)\n",
    "pos_arr[mask] = pos_arr[np.nonzero(mask)[0], idx[mask]]\n",
    "pl.DataFrame(pos_arr).write_csv(\"wfill.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame(pos_arr).to_pandas().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame(pos_beta_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.isnan(pos_arr)\n",
    "idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n",
    "np.maximum.accumulate(idx, axis=1, out=idx)\n",
    "pos_arr[mask] = pos_arr[np.nonzero(mask)[0], idx[mask]]\n",
    "pos_arr[np.isnan(pos_arr)] = 0\n",
    "\n",
    "pos_arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
